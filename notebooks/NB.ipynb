{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Naive bayes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The other day I had to prepare a class showing the benefits of using Naive Bayes. I have to say this is not a super powerful model, mainly because it makes assumptions that are most of the time not true. Nevertheless, I noticed this can e an excellent way to create a baseline model. It is easy, not very complicated to implement and the best thing is that is super fast.\n",
    "\n",
    "As you may know, this model is based on the Bayes Theorem, namely:\n",
    "\n",
    "$$P[B|A] = \\frac{P[A | B] \\cdot P[B]}{P[A]}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I´m not gonna demonstrate why this happens (basically because I don´t know how), but this models is based on probabilities that are calculated out of the dataset itself. In order to assign a class the class is calculated as:\n",
    "\n",
    "$$y = k = argmax\\, P[y = k] \\cdot \\prod{}_{i = 1}^p P[X_i/y = k]$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Where the class is denoted by the maximum probability (between the different classes) of the product of the a priori probability of y and the different likelihood of Variables $X_i$ given y = k. \n",
    "\n",
    "This example will be implemented using a lyrics dataset that I found [here](https://github.com/hiteshyalamanchili/SongGenreClassification/blob/master/dataset/english_cleaned_lyrics.zip). Shoutouts to Hitesh Yalamanchili for making this data available.\n",
    "\n",
    "So a naive Bayes model will be implemented in Python trying to Predict what genre a song belongs to by using its lyrics. SO the implementation in Python looks like this:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Importing Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When trying to import the data I noticed this has the following form:\n",
    "\n",
    "![](data_cap.PNG)\n",
    "\n",
    "For some reason there is a duplicated Index. In order to avoid a weird `Unnamed: 0` column I had to use `names` argument in `pd.read_csv()` to declare the actual column names to import. Even by doing that the DataFrame was imported as a double Index dataset so I had to remove one of the index using `.reset_index()`.\n",
    "\n",
    "> Note: In order to make the dataset manageable for demonstration purposes only I decided to use only four genres: Rock, Pop, Hip-Hop and Metal."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 3.32 s\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>song</th>\n",
       "      <th>year</th>\n",
       "      <th>artist</th>\n",
       "      <th>genre</th>\n",
       "      <th>lyrics</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ego-remix</td>\n",
       "      <td>2009</td>\n",
       "      <td>beyonce-knowles</td>\n",
       "      <td>Pop</td>\n",
       "      <td>Oh baby how you doing You know I'm gonna cut r...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>then-tell-me</td>\n",
       "      <td>2009</td>\n",
       "      <td>beyonce-knowles</td>\n",
       "      <td>Pop</td>\n",
       "      <td>playin everything so easy it's like you seem s...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>honesty</td>\n",
       "      <td>2009</td>\n",
       "      <td>beyonce-knowles</td>\n",
       "      <td>Pop</td>\n",
       "      <td>If you search For tenderness It isn't hard to ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>you-are-my-rock</td>\n",
       "      <td>2009</td>\n",
       "      <td>beyonce-knowles</td>\n",
       "      <td>Pop</td>\n",
       "      <td>Oh oh oh I oh oh oh I If I wrote a book about ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>black-culture</td>\n",
       "      <td>2009</td>\n",
       "      <td>beyonce-knowles</td>\n",
       "      <td>Pop</td>\n",
       "      <td>Party the people the people the party it's pop...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>362210</th>\n",
       "      <td>photographs-you-are-taking-now</td>\n",
       "      <td>2014</td>\n",
       "      <td>damon-albarn</td>\n",
       "      <td>Pop</td>\n",
       "      <td>When the photographs you're taking now Are tak...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>362211</th>\n",
       "      <td>you-and-me</td>\n",
       "      <td>2014</td>\n",
       "      <td>damon-albarn</td>\n",
       "      <td>Pop</td>\n",
       "      <td>I met Moko jumbie He walks on stilts through a...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>362212</th>\n",
       "      <td>hollow-ponds</td>\n",
       "      <td>2014</td>\n",
       "      <td>damon-albarn</td>\n",
       "      <td>Pop</td>\n",
       "      <td>Chill on the hollow ponds Set sail by a kid In...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>362213</th>\n",
       "      <td>the-selfish-giant</td>\n",
       "      <td>2014</td>\n",
       "      <td>damon-albarn</td>\n",
       "      <td>Pop</td>\n",
       "      <td>Celebrate the passing drugs Put them on the ba...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>362214</th>\n",
       "      <td>hostiles</td>\n",
       "      <td>2014</td>\n",
       "      <td>damon-albarn</td>\n",
       "      <td>Pop</td>\n",
       "      <td>When the serve is done And the parish shuffled...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>178054 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                  song  year           artist genre  \\\n",
       "0                            ego-remix  2009  beyonce-knowles   Pop   \n",
       "1                         then-tell-me  2009  beyonce-knowles   Pop   \n",
       "2                              honesty  2009  beyonce-knowles   Pop   \n",
       "3                      you-are-my-rock  2009  beyonce-knowles   Pop   \n",
       "4                        black-culture  2009  beyonce-knowles   Pop   \n",
       "...                                ...   ...              ...   ...   \n",
       "362210  photographs-you-are-taking-now  2014     damon-albarn   Pop   \n",
       "362211                      you-and-me  2014     damon-albarn   Pop   \n",
       "362212                    hollow-ponds  2014     damon-albarn   Pop   \n",
       "362213               the-selfish-giant  2014     damon-albarn   Pop   \n",
       "362214                        hostiles  2014     damon-albarn   Pop   \n",
       "\n",
       "                                                   lyrics  \n",
       "0       Oh baby how you doing You know I'm gonna cut r...  \n",
       "1       playin everything so easy it's like you seem s...  \n",
       "2       If you search For tenderness It isn't hard to ...  \n",
       "3       Oh oh oh I oh oh oh I If I wrote a book about ...  \n",
       "4       Party the people the people the party it's pop...  \n",
       "...                                                   ...  \n",
       "362210  When the photographs you're taking now Are tak...  \n",
       "362211  I met Moko jumbie He walks on stilts through a...  \n",
       "362212  Chill on the hollow ponds Set sail by a kid In...  \n",
       "362213  Celebrate the passing drugs Put them on the ba...  \n",
       "362214  When the serve is done And the parish shuffled...  \n",
       "\n",
       "[178054 rows x 5 columns]"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "import pandas as pd\n",
    "df = pd.read_csv('english_cleaned_lyrics.csv', header = 0, names = ['song','year','artist','genre', 'lyrics'], index_col = None).reset_index(level = 1, drop = True)\n",
    "df.query('genre in [\"Rock\",\"Pop\",\"Hip-Hop\",\"Metal\"]', inplace = True)\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Extraction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this step, we´ll use the `CountVectorizer()` class to provide a Occurrence Matrix. In this Matrix every row will be a Document, in this case a song, whereas every column is a Word. If a word ocurrs in the Document the is denoted by a 1. The only processing to the data is stopwords removal, that is removing all the words that are too common that end up adding noise to the analysis.\n",
    "\n",
    "We'll then use the word ocurrences as predictors for the genre. The predictor will look like this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 35.6 s\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>00</th>\n",
       "      <th>000</th>\n",
       "      <th>02</th>\n",
       "      <th>03</th>\n",
       "      <th>05</th>\n",
       "      <th>06</th>\n",
       "      <th>07</th>\n",
       "      <th>09</th>\n",
       "      <th>10</th>\n",
       "      <th>100</th>\n",
       "      <th>...</th>\n",
       "      <th>zones</th>\n",
       "      <th>zonin</th>\n",
       "      <th>zoo</th>\n",
       "      <th>zoom</th>\n",
       "      <th>zoomin</th>\n",
       "      <th>zoovie</th>\n",
       "      <th>zoovier</th>\n",
       "      <th>zoowap</th>\n",
       "      <th>zu</th>\n",
       "      <th>zulu</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>178049</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>178050</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>178051</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>178052</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>178053</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>178054 rows × 20000 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        00  000  02  03  05  06  07  09  10  100  ...  zones  zonin  zoo  \\\n",
       "0        0    0   0   0   0   0   0   0   0    0  ...      0      0    0   \n",
       "1        0    0   0   0   0   0   0   0   0    0  ...      0      0    0   \n",
       "2        0    0   0   0   0   0   0   0   0    0  ...      0      0    0   \n",
       "3        0    0   0   0   0   0   0   0   0    0  ...      0      0    0   \n",
       "4        0    0   0   0   0   0   0   0   0    0  ...      0      0    0   \n",
       "...     ..  ...  ..  ..  ..  ..  ..  ..  ..  ...  ...    ...    ...  ...   \n",
       "178049   0    0   0   0   0   0   0   0   0    0  ...      0      0    0   \n",
       "178050   0    0   0   0   0   0   0   0   0    0  ...      0      0    0   \n",
       "178051   0    0   0   0   0   0   0   0   0    0  ...      0      0    0   \n",
       "178052   0    0   0   0   0   0   0   0   0    0  ...      0      0    0   \n",
       "178053   0    0   0   0   0   0   0   0   0    0  ...      0      0    0   \n",
       "\n",
       "        zoom  zoomin  zoovie  zoovier  zoowap  zu  zulu  \n",
       "0          0       0       0        0       0   0     0  \n",
       "1          0       0       0        0       0   0     0  \n",
       "2          0       0       0        0       0   0     0  \n",
       "3          0       0       0        0       0   0     0  \n",
       "4          0       0       0        0       0   0     0  \n",
       "...      ...     ...     ...      ...     ...  ..   ...  \n",
       "178049     0       0       0        0       0   0     0  \n",
       "178050     0       0       0        0       0   0     0  \n",
       "178051     0       0       0        0       0   0     0  \n",
       "178052     0       0       0        0       0   0     0  \n",
       "178053     0       0       0        0       0   0     0  \n",
       "\n",
       "[178054 rows x 20000 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "c_vec = CountVectorizer(stop_words = 'english', max_features = 20000) ## I´m removing english stopwords, and setting the max number of predictors to 20000 to avoid my computer to crush.\n",
    "vectorizer = c_vec.fit_transform(df['lyrics']) \n",
    "# Transform output into pandas Df for visualization\n",
    "pd.DataFrame(vectorizer.toarray(), columns = c_vec.get_feature_names()) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setting up the Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The model is usper easy to set up. We just need to import `MultinomialNB` since this is a multiclass Prediction Model. Additionally, we´ll import `train_test_split()` to split the data into train and test, `Pipeline()` to create the Model Pipeline (the steps to come up with the model) and `classification_report()` to measure model performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.metrics import confusion_matrix, classification_report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# X will be song lyrics and y is the genre. We´ll split the data using 40% for test purposes.\n",
    "X_train, X_test, y_train, y_test = train_test_split(df['lyrics'], df['genre'], test_size = 0.4, random_state = 123) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then a pipeline will be set using 2 steps. The first one being the `CountVectorizer()` named 'cv' and the `MultinomialNB()` model named 'nb':"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 26.4 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "text_clf = Pipeline(steps = [\n",
    "    ('cv', CountVectorizer(stop_words = 'english', max_features = 20000)),\n",
    "    ('nb', MultinomialNB(alpha = 0.1))\n",
    "])\n",
    "text_clf.fit(X_train, y_train) # fitting the Pipeline\n",
    "#predicting using the Test Set to measure performance\n",
    "y_pred = text_clf.predict(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first to notice is that even having 178K rows and 20000 predictors the model fits in under 30 seconds.\n",
    "\n",
    "Now when it comes to results, It is not a terrible model, it has a 63% od accuracy and the Macro F1 is 62%. Not bad for just using a couple of lines of code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "     Hip-Hop       0.72      0.77      0.74      9062\n",
      "       Metal       0.48      0.75      0.59      8551\n",
      "         Pop       0.42      0.53      0.47     13582\n",
      "        Rock       0.78      0.60      0.68     40027\n",
      "\n",
      "    accuracy                           0.63     71222\n",
      "   macro avg       0.60      0.66      0.62     71222\n",
      "weighted avg       0.66      0.63      0.63     71222\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(y_test,y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Improving the Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to improve the model, we could run a GridSearch trying to play around with the alpha smoothing parameter that NB has. By adjusting this parameter correctly we could easily improve a bit the model without too much effort.\n",
    "\n",
    "In this case we´ll run a Grid using values from 0 to 1, as shown below. Additionally we´ll use the 'f1_macro' as the metric to choose the best model using a 5 Fold Cross Validation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 3min 27s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=5,\n",
       "             estimator=Pipeline(steps=[('cv',\n",
       "                                        CountVectorizer(stop_words='english')),\n",
       "                                       ('nb', MultinomialNB())]),\n",
       "             n_jobs=-1, param_grid={'nb__alpha': [0, 0.001, 0.01, 0.1, 0.5, 1]},\n",
       "             scoring='f1_macro')"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "from sklearn import set_config\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import precision_score, make_scorer\n",
    "#Grilla de parámetros a buscar\n",
    "parameters = {'nb__alpha': [0, 0.001, 0.01, 0.1, 0.5, 1] }\n",
    "\n",
    "text_clf = Pipeline(steps = [\n",
    "    ('cv', CountVectorizer(stop_words = 'english')),\n",
    "    ('nb', MultinomialNB())\n",
    "])\n",
    "\n",
    "\n",
    "searchCV = GridSearchCV(text_clf, parameters, n_jobs = -1, scoring = 'f1_macro', cv = 5) # 5 Fold CV optimizando el modelo por f1 macro\n",
    "searchCV.fit(X_train, y_train)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The GridSearch takes around 3 minutes to run 6 models using 5-Fold CV. And we can inmediately notice, small improvements for the best model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "     Hip-Hop       0.73      0.77      0.75      9062\n",
      "       Metal       0.56      0.70      0.62      8551\n",
      "         Pop       0.45      0.49      0.47     13582\n",
      "        Rock       0.76      0.69      0.73     40027\n",
      "\n",
      "    accuracy                           0.66     71222\n",
      "   macro avg       0.63      0.66      0.64     71222\n",
      "weighted avg       0.67      0.66      0.67     71222\n",
      "\n"
     ]
    }
   ],
   "source": [
    "best_nb = searchCV.best_estimator_  # Extracting Best Model\n",
    "y_pred = best_nb.predict(X_test) # Predicting the Test Set\n",
    "print(classification_report(y_test,y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can inmediately see that:\n",
    "\n",
    "* Overall Accuracy improves 3%.\n",
    "* F1 macro average improved 2%. \n",
    "* The Rock category is the one that improves the most from 68 to 73%.\n",
    "* There is a trade off, even though some classess improve we can see that Metal decreases, whereas Pop keep the same results."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally we can check that the best is achieved when using alpha equals to 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'alpha': 1, 'class_prior': None, 'fit_prior': True}"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best_nb.named_steps.nb.get_params()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is just a short example on how to set up a baseline model. Hopefully this can be useful for you.\n",
    "\n",
    "See you next time!!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7.7 64-bit ('MLprojects': conda)",
   "language": "python",
   "name": "python37764bitmlprojectsconda9e66019a9ab047499c0882be49df755b"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
